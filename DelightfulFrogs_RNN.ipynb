{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09eeeR5prIJ"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCCk8_dHpuNf"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE, THE DELIGHTFUL FROGS, BASED THE FOLLOWING CODE OFF TENSORFLOW'S TUTORIAL HERE:\n",
        "\n",
        "https://www.tensorflow.org/text/tutorials/text_generation"
      ],
      "metadata": {
        "id": "qPlEcMw9u1JG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setting up the hyperparameters\n",
        "\n",
        "If you want to change the model used, see section 'Build the Model'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_input = 'scripts_all.txt'\n",
        "path_to_output = 'all_e30_s30.txt'\n",
        "model_save_file = 'one_step_all_e30_s30'\n",
        "checkpoint_dir = './training_checkpoints_all_e30_s30'\n",
        "\n",
        "seq_length = 30     # Predict the next char based on the last x char\n",
        "\n",
        "embedding_dim = 256 # The embedding dimension\n",
        "rnn_units = 1024    # Number of RNN units\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# How many times we go over the training data\n",
        "EPOCHS = 30\n",
        "\n",
        "# The starting data to predict on\n",
        "start_chars = 'FROGS:'\n",
        "# How many thousand char to predict\n",
        "kthousand = 12"
      ],
      "metadata": {
        "id": "7AZ2BRjBsmjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "## Importing TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "## Reading in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavnuByVymwK",
        "outputId": "dd2dec57-9769-400a-ce1b-821a8ec74317",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 871612 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_input, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "outputId": "df31a99a-f565-47bf-8ab5-d6d039d71114",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALOIS:\n",
            "Darkness... Darkness... Darkness. Darkness is coiling itself around me... I am ensnared by the darkness, and every last drop of my blood will be drained away by the blade piercing me. \n",
            "\n",
            "CLAUDE:\n",
            "That said... \n",
            "\n",
            "ALOIS:\n",
            "That said, I still want thi\n"
          ]
        }
      ],
      "source": [
        "# Peak at the first 250 characters\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "outputId": "c57dc0ee-b249-41aa-a6e4-3d49cd015fe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "vocab_size = len(vocab)\n",
        "print(f'{vocab_size} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Processing the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorizing the text\n",
        "\n",
        "Before training, we need the chars in a numerical representation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenivzwqsDhp"
      },
      "source": [
        "And we need to do the reverse, which is going from ids to characters. Also for easier human reading,we want to be able to go from characters to a string of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UopbsKi88tm5",
        "outputId": "8b956831-b593-494d-8ae3-4ba60503a88f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "L\n",
            "O\n",
            "I\n",
            "S\n",
            ":\n",
            "\n",
            "\n",
            "D\n",
            "a\n",
            "r\n"
          ]
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids\n",
        "\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Creating training examples and targets\n",
        "\n",
        "We need to divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "The `batch` method easily converts these individual characters to sequences of the desired  size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpdjRO2CzOfZ"
      },
      "outputs": [],
      "source": [
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right (so that it knows what the next char should be).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9iKPXkw5xwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8845eed-8076-4c6a-b636-05c36629f732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'ALOIS:\\nDarkness... Darkness...'\n",
            "Target: b'LOIS:\\nDarkness... Darkness... '\n"
          ]
        }
      ],
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Creating training batches\n",
        "\n",
        "We need to shuffle the sequences and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2pGotuNzf-S",
        "outputId": "320f05bd-4a86-4281-e1cf-d200643b169f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 30), dtype=tf.int64, name=None), TensorSpec(shape=(64, 30), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Building The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.\n",
        "\n",
        "If we wanted, we could change the model to be any series of layers that we want with other possibilities being RNN, LSTM, alternating layers, etc. Then using keras.Sequential would be the method to use for the construction of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.algorithm = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.algorithm.get_initial_state(x)\n",
        "    x, states = self.algorithm(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEaZC7EGtpst",
        "outputId": "76829bb5-2318-4022-ac84-879161d47894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 30, 92) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     multiple                  23552     \n",
            "                                                                 \n",
            " gru_4 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_4 (Dense)             multiple                  94300     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,056,156\n",
            "Trainable params: 4,056,156\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "unmodified from the tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attaching an optimizer, and a loss function\n",
        "\n",
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOeWdgxNFDXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91345f2e-fdbc-4bbc-9d98-04a1b3d2dcee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 30, 92)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.5221605, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3008b32f-757d-49cb-9e00-26ef30e3728f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92.034225"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configuring save checkpoints for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "###  Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGBE2zxMMHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a85c589-c876-49ad-b303-4f94c6e2a082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "439/439 [==============================] - 10s 16ms/step - loss: 2.1711\n",
            "Epoch 2/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 1.5715\n",
            "Epoch 3/30\n",
            "439/439 [==============================] - 8s 17ms/step - loss: 1.3924\n",
            "Epoch 4/30\n",
            "439/439 [==============================] - 8s 16ms/step - loss: 1.2967\n",
            "Epoch 5/30\n",
            "439/439 [==============================] - 8s 17ms/step - loss: 1.2254\n",
            "Epoch 6/30\n",
            "439/439 [==============================] - 8s 17ms/step - loss: 1.1614\n",
            "Epoch 7/30\n",
            "439/439 [==============================] - 8s 17ms/step - loss: 1.1003\n",
            "Epoch 8/30\n",
            "439/439 [==============================] - 8s 17ms/step - loss: 1.0403\n",
            "Epoch 9/30\n",
            "439/439 [==============================] - 8s 17ms/step - loss: 0.9811\n",
            "Epoch 10/30\n",
            "439/439 [==============================] - 8s 17ms/step - loss: 0.9228\n",
            "Epoch 11/30\n",
            "439/439 [==============================] - 8s 17ms/step - loss: 0.8691\n",
            "Epoch 12/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.8195\n",
            "Epoch 13/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.7743\n",
            "Epoch 14/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.7354\n",
            "Epoch 15/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.7044\n",
            "Epoch 16/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.6783\n",
            "Epoch 17/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.6567\n",
            "Epoch 18/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.6380\n",
            "Epoch 19/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.6259\n",
            "Epoch 20/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.6143\n",
            "Epoch 21/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.6066\n",
            "Epoch 22/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.6020\n",
            "Epoch 23/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.5982\n",
            "Epoch 24/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.5974\n",
            "Epoch 25/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.5952\n",
            "Epoch 26/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.5915\n",
            "Epoch 27/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.5896\n",
            "Epoch 28/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.5914\n",
            "Epoch 29/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.5947\n",
            "Epoch 30/30\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.5944\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generating text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The following is to generate a 1 character at a time and using that generated character to make the ones that follow. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now telling it to actually do that generation."
      ],
      "metadata": {
        "id": "B5vMiJK-vTZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "start_chars='FROGS:'\n",
        "next_char = tf.constant([start_chars])\n",
        "filewrite = open(path_to_output, 'a')\n",
        "\n",
        "for i in range(kthousand+1):\n",
        "    result = [next_char]\n",
        "\n",
        "    for n in range(1000): \n",
        "      next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "      result.append(next_char)\n",
        "\n",
        "      if i==kthousand and next_char=='.':\n",
        "          break\n",
        "    #if i==kthousand\n",
        "\n",
        "    result = tf.strings.join(result)\n",
        "\n",
        "    a = result[0].numpy().decode('utf-8')\n",
        "    filewrite.write(a)\n",
        "    next_char = tf.constant([a[-10:]])\n",
        "\n",
        "filewrite.close()\n",
        "\n",
        "end = time.time()\n",
        "#print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n"
      ],
      "metadata": {
        "id": "tOdLRsaAFsTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0318ec-e552-4418-dfa3-6b7bd9ee423d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run time: 28.6715350151062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## Export the generator\n",
        "\n",
        "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing you to use it anywhere a `tf.saved_model` is accepted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Grk32H_CzsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aab028c-3ac0-48d2-dcde-c5b786f3af71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f187e32a510>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: one_step_all_e30_s30/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: one_step_all_e30_s30/assets\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, model_save_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z9bb_wX6Uuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "b565a115-9e45-4b17-8d15-29d02a0f31e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\npath_to_output = 'action_e30_s50.txt'\\none_step_reloaded = tf.saved_model.load('one_step_action_e30_s50')\\n\\nstart = time.time()\\nstates = None\\nstart_chars='FROGS:'\\nnext_char = tf.constant([start_chars])\\nfilewrite = open(path_to_output, 'a')\\n\\nfor i in range(kthousand+1):\\n    result = [next_char]\\n\\n    for n in range(1000): \\n      next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\\n      result.append(next_char)\\n\\n      if i==kthousand and next_char=='.':\\n          break\\n\\n    result = tf.strings.join(result)\\n\\n    a = result[0].numpy().decode('utf-8')\\n    filewrite.write(a)\\n    next_char = tf.constant([a[-10:]])\\n\\nfilewrite.close()\\n\\nend = time.time()\\n#print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\\nprint('\\nRun time:', end - start)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "\"\"\"\n",
        "path_to_output = 'action_e30_s50.txt'\n",
        "one_step_reloaded = tf.saved_model.load('one_step_action_e30_s50')\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "start_chars='FROGS:'\n",
        "next_char = tf.constant([start_chars])\n",
        "filewrite = open(path_to_output, 'a')\n",
        "\n",
        "for i in range(kthousand+1):\n",
        "    result = [next_char]\n",
        "\n",
        "    for n in range(1000): \n",
        "      next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "      result.append(next_char)\n",
        "\n",
        "      if i==kthousand and next_char=='.':\n",
        "          break\n",
        "\n",
        "    result = tf.strings.join(result)\n",
        "\n",
        "    a = result[0].numpy().decode('utf-8')\n",
        "    filewrite.write(a)\n",
        "    next_char = tf.constant([a[-10:]])\n",
        "\n",
        "filewrite.close()\n",
        "\n",
        "end = time.time()\n",
        "#print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MoIe8Cca2EMe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DelightfulFrogs_RNNv3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}